{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a506457",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# DeepFont Models Visualized\n",
    "\n",
    "This notebook provides visualizations of the DeepFont model architectures, including both the autoencoder (DeepFontAE) and the classifier (DeepFont). The visualizations help in understanding the structure and flow of data through the models.\n",
    "\n",
    "In the original paper, the exact architectures were not explicitly detailed and needed to be inferred from the text and figures provided. This notebook aims to clarify those architectures through visual representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a549dd0",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We will visualize two models:\n",
    "1. **DeepFontAE**: The autoencoder used for unsupervised pretraining\n",
    "2. **DeepFont**: The full classification model (which can use pretrained encoder weights)\n",
    "\n",
    "Both models will be visualized to show:\n",
    "- Layer types and dimensions\n",
    "- Input/output shapes at each stage\n",
    "- Parameter counts\n",
    "- Overall architecture flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bf4a2b",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Dependencies\n",
    "\n",
    "Install and import necessary libraries for visualization:\n",
    "- `torchinfo` or `torchsummary` for model summaries\n",
    "- `graphviz` and `torchviz` for computational graph visualization (optional)\n",
    "- Standard PyTorch utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9d9797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "\n",
    "from deepfont.models.deepfont import DeepFont, DeepFontAE\n",
    "\n",
    "# Create assets directory\n",
    "assets_dir = Path(\"../assets\")\n",
    "assets_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1198cc3d",
   "metadata": {},
   "source": [
    "## Step 2: Instantiate Models\n",
    "\n",
    "Create instances of both models:\n",
    "- `DeepFontAE`: Autoencoder with optional output activation\n",
    "- `DeepFont`: Classifier with specified number of output classes\n",
    "\n",
    "We'll use a representative number of classes (e.g., 2383 fonts from the paper) for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d65385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the autoencoder\n",
    "ae_model = DeepFontAE(output_activation=\"sigmoid\")\n",
    "\n",
    "# Instantiate the classifier with 2383 classes (from the paper)\n",
    "classifier_model = DeepFont(num_out=2383)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b7bd7d",
   "metadata": {},
   "source": [
    "## Step 3: DeepFontAE Architecture Visualization\n",
    "\n",
    "Visualize the autoencoder architecture:\n",
    "\n",
    "### 3.1 Model Summary\n",
    "Display a detailed summary showing:\n",
    "- Layer names and types\n",
    "- Output shapes at each layer\n",
    "- Number of parameters (trainable and total)\n",
    "- Model input size (105×105 grayscale images)\n",
    "\n",
    "### 3.2 Encoder-Decoder Structure\n",
    "Show the encoder and decoder separately:\n",
    "- **Encoder**: Conv layers with max pooling for compression\n",
    "- **Decoder**: Transposed convolutions and upsampling for reconstruction\n",
    "\n",
    "### 3.3 Information Flow\n",
    "Illustrate how the spatial dimensions change through the network:\n",
    "- Input: (batch_size, 1, 105, 105)\n",
    "- Latent representation shape\n",
    "- Output: (batch_size, 1, 105, 105) - reconstructed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4706dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive model summary\n",
    "print(\"=\" * 80)\n",
    "print(\"DeepFontAE - Complete Architecture\")\n",
    "print(\"=\" * 80)\n",
    "ae_summary = summary(\n",
    "    ae_model,\n",
    "    input_size=(1, 1, 105, 105),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\"],\n",
    "    depth=5,\n",
    "    verbose=0,\n",
    ")\n",
    "print(ae_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize encoder and decoder separately\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DeepFontAE - Encoder\")\n",
    "print(\"=\" * 80)\n",
    "encoder_summary = summary(\n",
    "    ae_model.encoder,\n",
    "    input_size=(1, 1, 105, 105),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "    depth=5,\n",
    "    verbose=0,\n",
    ")\n",
    "print(encoder_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DeepFontAE - Decoder\")\n",
    "print(\"=\" * 80)\n",
    "# Get encoder output shape to use as decoder input\n",
    "device = next(ae_model.parameters()).device\n",
    "with torch.no_grad():\n",
    "    encoder_out = ae_model.encoder(torch.randn(1, 1, 105, 105, device=device))\n",
    "    decoder_input_shape = encoder_out.shape\n",
    "\n",
    "decoder_summary = summary(\n",
    "    ae_model.decoder,\n",
    "    input_size=decoder_input_shape,\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "    depth=5,\n",
    "    verbose=0,\n",
    ")\n",
    "print(decoder_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ada4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visual diagram of information flow through DeepFontAE\n",
    "fig, ax = plt.subplots(figsize=(18, 7))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# Define layer information with uniform spacing\n",
    "layer_names = [\n",
    "    (\"Input\", \"(1, 1, 105, 105)\"),\n",
    "    (\"Conv2d\\n(64, 11x11, s=2)\", \"(1, 64, 48, 48)\"),\n",
    "    (\"MaxPool2d\\n(2x2)\", \"(1, 64, 24, 24)\"),\n",
    "    (\"ReLU\", \"(1, 64, 24, 24)\"),\n",
    "    (\"Conv2d\\n(128, 5x5)\", \"(1, 128, 24, 24)\"),\n",
    "    (\"MaxPool2d\\n(2x2)\", \"(1, 128, 12, 12)\"),\n",
    "    (\"ReLU\\nLatent\", \"(1, 128, 12, 12)\"),\n",
    "    (\"ConvTranspose2d\\n(64, 5x5, s=2)\", \"(1, 64, 24, 24)\"),\n",
    "    (\"ReLU\", \"(1, 64, 24, 24)\"),\n",
    "    (\"Upsample\\n(scale=2)\", \"(1, 64, 48, 48)\"),\n",
    "    (\"ConvTranspose2d\\n(1, 11x11, s=2)\", \"(1, 1, 105, 105)\"),\n",
    "    (\"Sigmoid\\nOutput\", \"(1, 1, 105, 105)\"),\n",
    "]\n",
    "\n",
    "# Calculate uniform x positions\n",
    "x_start = 0.06\n",
    "x_end = 0.97\n",
    "num_layers = len(layer_names)\n",
    "x_positions = np.linspace(x_start, x_end, num_layers)\n",
    "\n",
    "# Combine names with positions\n",
    "layers = [(name, shape, x_pos) for (name, shape), x_pos in zip(layer_names, x_positions)]\n",
    "\n",
    "# Draw boxes and arrows\n",
    "y_pos = 0.5\n",
    "box_height = 0.10\n",
    "box_width = 0.06\n",
    "encoder_color = \"#E8F4F8\"\n",
    "decoder_color = \"#F8E8E8\"\n",
    "latent_color = \"#FFF4E8\"\n",
    "\n",
    "for i, (name, shape, x_pos) in enumerate(layers):\n",
    "    # Determine color based on position\n",
    "    if i < 7:  # Encoder + latent\n",
    "        color = encoder_color if i < 6 else latent_color\n",
    "    else:  # Decoder\n",
    "        color = decoder_color\n",
    "\n",
    "    # Draw box\n",
    "    box = FancyBboxPatch(\n",
    "        (x_pos - box_width / 2, y_pos - box_height / 2),\n",
    "        box_width,\n",
    "        box_height,\n",
    "        boxstyle=\"round,pad=0.005\",\n",
    "        edgecolor=\"black\",\n",
    "        facecolor=color,\n",
    "        linewidth=2,\n",
    "    )\n",
    "    ax.add_patch(box)\n",
    "\n",
    "    # Add text\n",
    "    ax.text(x_pos, y_pos + 0.02, name, ha=\"center\", va=\"center\", fontsize=8, fontweight=\"bold\")\n",
    "    ax.text(x_pos, y_pos - 0.02, shape, ha=\"center\", va=\"center\", fontsize=6.5, style=\"italic\")\n",
    "\n",
    "    # Draw arrow to next layer\n",
    "    if i < len(layers) - 1:\n",
    "        next_x = layers[i + 1][2]\n",
    "        ax.annotate(\n",
    "            \"\",\n",
    "            xy=(next_x - box_width / 2, y_pos),\n",
    "            xytext=(x_pos + box_width / 2, y_pos),\n",
    "            arrowprops=dict(arrowstyle=\"->\", lw=2, color=\"black\"),\n",
    "        )\n",
    "\n",
    "# Add labels centered above each section, closer to the boxes\n",
    "# Calculate center positions for each group\n",
    "encoder_center = np.mean([layers[i][2] for i in range(6)])  # First 6 boxes\n",
    "latent_center = layers[6][2]  # Single latent box\n",
    "decoder_center = np.mean([layers[i][2] for i in range(7, 12)])  # Last 5 boxes\n",
    "\n",
    "label_y = 0.63  # Closer to boxes (was 0.68)\n",
    "\n",
    "ax.text(\n",
    "    encoder_center,\n",
    "    label_y,\n",
    "    \"ENCODER\",\n",
    "    fontsize=13,\n",
    "    fontweight=\"bold\",\n",
    "    ha=\"center\",\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=encoder_color, edgecolor=\"black\", linewidth=2),\n",
    ")\n",
    "ax.text(\n",
    "    latent_center,\n",
    "    label_y,\n",
    "    \"LATENT\",\n",
    "    fontsize=13,\n",
    "    fontweight=\"bold\",\n",
    "    ha=\"center\",\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=latent_color, edgecolor=\"black\", linewidth=2),\n",
    ")\n",
    "ax.text(\n",
    "    decoder_center,\n",
    "    label_y,\n",
    "    \"DECODER\",\n",
    "    fontsize=13,\n",
    "    fontweight=\"bold\",\n",
    "    ha=\"center\",\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=decoder_color, edgecolor=\"black\", linewidth=2),\n",
    ")\n",
    "\n",
    "ax.set_xlim(0, 1.03)\n",
    "ax.set_ylim(0.35, 0.75)\n",
    "\n",
    "plt.title(\"DeepFontAE Architecture - Information Flow\", fontsize=16, fontweight=\"bold\", pad=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "save_path = assets_dir / \"deepfontae_architecture.png\"\n",
    "plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Saved visualization to: {save_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d7c03",
   "metadata": {},
   "source": [
    "## Step 4: DeepFont Classifier Architecture Visualization\n",
    "\n",
    "Visualize the full classification model:\n",
    "\n",
    "### 4.1 Complete Model Summary\n",
    "Display the full architecture with all three parts:\n",
    "- **Encoder** (2 conv layers with batch norm)\n",
    "- **Conv Part** (3 additional conv layers)\n",
    "- **FC Part** (3 fully connected layers with dropout)\n",
    "\n",
    "### 4.2 Three-Stage Architecture\n",
    "Break down visualization by stages:\n",
    "1. **Feature Extraction (Encoder)**: Initial convolutional layers\n",
    "2. **Feature Refinement (Conv Part)**: Deeper convolutional processing\n",
    "3. **Classification (FC Part)**: Fully connected layers to class logits\n",
    "\n",
    "### 4.3 Key Architectural Details\n",
    "Highlight important aspects:\n",
    "- Batch normalization in encoder (vs. no batch norm in autoencoder)\n",
    "- Same convolutions (padding=\"same\") in conv_part\n",
    "- Dropout layers (0.1) for regularization\n",
    "- Output dimensions (num_out classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dc1277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive model summary for classifier\n",
    "print(\"=\" * 80)\n",
    "print(\"DeepFont Classifier - Complete Architecture\")\n",
    "print(\"=\" * 80)\n",
    "classifier_summary = summary(\n",
    "    classifier_model,\n",
    "    input_size=(1, 1, 105, 105),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\"],\n",
    "    depth=5,\n",
    "    verbose=0,\n",
    ")\n",
    "print(classifier_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951e49d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the three stages separately\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DeepFont - Stage 1: Encoder (Feature Extraction)\")\n",
    "print(\"=\" * 80)\n",
    "encoder_clf_summary = summary(\n",
    "    classifier_model.encoder,\n",
    "    input_size=(1, 1, 105, 105),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "    depth=5,\n",
    "    verbose=0,\n",
    ")\n",
    "print(encoder_clf_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DeepFont - Stage 2: Conv Part (Feature Refinement)\")\n",
    "print(\"=\" * 80)\n",
    "# Get encoder output shape\n",
    "device = next(classifier_model.parameters()).device\n",
    "with torch.no_grad():\n",
    "    encoder_clf_out = classifier_model.encoder(torch.randn(1, 1, 105, 105, device=device))\n",
    "    conv_input_shape = encoder_clf_out.shape\n",
    "\n",
    "conv_part_summary = summary(\n",
    "    classifier_model.conv_part,\n",
    "    input_size=conv_input_shape,\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "    depth=5,\n",
    "    verbose=0,\n",
    ")\n",
    "print(conv_part_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DeepFont - Stage 3: FC Part (Classification)\")\n",
    "print(\"=\" * 80)\n",
    "# Get conv_part output shape\n",
    "with torch.no_grad():\n",
    "    conv_out = classifier_model.conv_part(encoder_clf_out)\n",
    "    fc_input_shape = conv_out.shape\n",
    "\n",
    "fc_part_summary = summary(\n",
    "    classifier_model.fc_part,\n",
    "    input_size=fc_input_shape,\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "    depth=5,\n",
    "    verbose=0,\n",
    ")\n",
    "print(fc_part_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a1707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visual diagram of DeepFont classifier architecture\n",
    "fig, ax = plt.subplots(figsize=(24, 7))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# Define stages with uniform spacing\n",
    "stage_info = [\n",
    "    # Encoder\n",
    "    (\"Input\", \"(1, 1, 105, 105)\", \"encoder\"),\n",
    "    (\"Conv2d+BN\\n(64, 11x11, s=2)\", \"(1, 64, 48, 48)\", \"encoder\"),\n",
    "    (\"MaxPool2d\\n(2x2)\", \"(1, 64, 24, 24)\", \"encoder\"),\n",
    "    (\"ReLU\", \"(1, 64, 24, 24)\", \"encoder\"),\n",
    "    (\"Conv2d+BN\\n(128, 5x5)\", \"(1, 128, 24, 24)\", \"encoder\"),\n",
    "    (\"MaxPool2d\\n(2x2)\", \"(1, 128, 12, 12)\", \"encoder\"),\n",
    "    (\"ReLU\", \"(1, 128, 12, 12)\", \"encoder\"),\n",
    "    # Conv Part\n",
    "    (\"Conv2d+BN\\n(256, 3x3)\", \"(1, 256, 12, 12)\", \"conv\"),\n",
    "    (\"ReLU\", \"(1, 256, 12, 12)\", \"conv\"),\n",
    "    (\"Conv2d+BN\\n(256, 3x3)\", \"(1, 256, 12, 12)\", \"conv\"),\n",
    "    (\"ReLU\", \"(1, 256, 12, 12)\", \"conv\"),\n",
    "    (\"Conv2d+BN\\n(256, 3x3)\", \"(1, 256, 12, 12)\", \"conv\"),\n",
    "    (\"ReLU\", \"(1, 256, 12, 12)\", \"conv\"),\n",
    "    # FC Part\n",
    "    (\"Flatten\", \"(1, 36864)\", \"fc\"),\n",
    "    (\"Linear\\n(4096)\", \"(1, 4096)\", \"fc\"),\n",
    "    (\"Dropout+ReLU\", \"(1, 4096)\", \"fc\"),\n",
    "    (\"Linear\\n(4096)\", \"(1, 4096)\", \"fc\"),\n",
    "    (\"Dropout+ReLU\", \"(1, 4096)\", \"fc\"),\n",
    "    (\"Linear\\n(2383)\", \"(1, 2383)\", \"fc\"),\n",
    "]\n",
    "\n",
    "# Calculate uniform x positions\n",
    "x_start = 0.05\n",
    "x_end = 0.98\n",
    "num_stages = len(stage_info)\n",
    "x_positions = np.linspace(x_start, x_end, num_stages)\n",
    "\n",
    "# Combine with positions\n",
    "stages = [\n",
    "    (name, shape, x_pos, stage) for (name, shape, stage), x_pos in zip(stage_info, x_positions)\n",
    "]\n",
    "\n",
    "# Colors for each stage\n",
    "stage_colors = {\"encoder\": \"#E8F4F8\", \"conv\": \"#E8F8E8\", \"fc\": \"#F8E8F8\"}\n",
    "\n",
    "# Draw layers\n",
    "y_pos = 0.5\n",
    "box_height = 0.08\n",
    "box_width = 0.042\n",
    "\n",
    "for i, (name, shape, x_pos, stage) in enumerate(stages):\n",
    "    # Draw box\n",
    "    color = stage_colors[stage]\n",
    "    box = FancyBboxPatch(\n",
    "        (x_pos - box_width / 2, y_pos - box_height / 2),\n",
    "        box_width,\n",
    "        box_height,\n",
    "        boxstyle=\"round,pad=0.003\",\n",
    "        edgecolor=\"black\",\n",
    "        facecolor=color,\n",
    "        linewidth=1.5,\n",
    "    )\n",
    "    ax.add_patch(box)\n",
    "\n",
    "    # Add text\n",
    "    ax.text(x_pos, y_pos + 0.02, name, ha=\"center\", va=\"center\", fontsize=8, fontweight=\"bold\")\n",
    "    ax.text(x_pos, y_pos - 0.02, shape, ha=\"center\", va=\"center\", fontsize=6.5, style=\"italic\")\n",
    "\n",
    "    # Draw arrow to next layer\n",
    "    if i < len(stages) - 1:\n",
    "        next_x = stages[i + 1][2]\n",
    "        arrow_style = \"->\" if stages[i][3] == stages[i + 1][3] else \"-|>\"\n",
    "        ax.annotate(\n",
    "            \"\",\n",
    "            xy=(next_x - box_width / 2, y_pos),\n",
    "            xytext=(x_pos + box_width / 2, y_pos),\n",
    "            arrowprops=dict(arrowstyle=arrow_style, lw=1.5, color=\"black\"),\n",
    "        )\n",
    "\n",
    "# Add stage labels centered above each section, closer to the boxes\n",
    "# Calculate center positions for each group\n",
    "encoder_indices = [i for i, (_, _, _, stage) in enumerate(stages) if stage == \"encoder\"]\n",
    "conv_indices = [i for i, (_, _, _, stage) in enumerate(stages) if stage == \"conv\"]\n",
    "fc_indices = [i for i, (_, _, _, stage) in enumerate(stages) if stage == \"fc\"]\n",
    "\n",
    "encoder_center = np.mean([stages[i][2] for i in encoder_indices])\n",
    "conv_center = np.mean([stages[i][2] for i in conv_indices])\n",
    "fc_center = np.mean([stages[i][2] for i in fc_indices])\n",
    "\n",
    "label_y = 0.63  # Closer to boxes\n",
    "\n",
    "# Add stage labels\n",
    "ax.text(\n",
    "    encoder_center,\n",
    "    label_y,\n",
    "    \"ENCODER\\n(Feature Extraction)\",\n",
    "    fontsize=11,\n",
    "    fontweight=\"bold\",\n",
    "    ha=\"center\",\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=stage_colors[\"encoder\"], edgecolor=\"black\", linewidth=2),\n",
    ")\n",
    "ax.text(\n",
    "    conv_center,\n",
    "    label_y,\n",
    "    \"CONV PART\\n(Feature Refinement)\",\n",
    "    fontsize=11,\n",
    "    fontweight=\"bold\",\n",
    "    ha=\"center\",\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=stage_colors[\"conv\"], edgecolor=\"black\", linewidth=2),\n",
    ")\n",
    "ax.text(\n",
    "    fc_center,\n",
    "    label_y,\n",
    "    \"FC PART\\n(Classification)\",\n",
    "    fontsize=11,\n",
    "    fontweight=\"bold\",\n",
    "    ha=\"center\",\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=stage_colors[\"fc\"], edgecolor=\"black\", linewidth=2),\n",
    ")\n",
    "\n",
    "ax.set_xlim(0, 1.03)\n",
    "ax.set_ylim(0.35, 0.75)\n",
    "\n",
    "plt.title(\n",
    "    \"DeepFont Classifier Architecture - Complete Pipeline\", fontsize=16, fontweight=\"bold\", pad=20\n",
    ")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "save_path = assets_dir / \"deepfont_classifier_architecture.png\"\n",
    "plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Saved visualization to: {save_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492efd17",
   "metadata": {},
   "source": [
    "## Step 5: Comparison and Transfer Learning\n",
    "\n",
    "### 5.1 Encoder Comparison\n",
    "Compare the encoder architectures between DeepFontAE and DeepFont:\n",
    "- Highlight similarities (conv layer structure)\n",
    "- Highlight differences (batch normalization)\n",
    "- Show parameter mapping for transfer learning\n",
    "\n",
    "### 5.2 Parameter Statistics\n",
    "Display comparative statistics:\n",
    "- Total parameters in each model\n",
    "- Trainable vs. frozen parameters (when using pretrained encoder)\n",
    "- Memory requirements\n",
    "\n",
    "### 5.3 Transfer Learning Visualization\n",
    "Illustrate the transfer learning workflow:\n",
    "1. Pretrain DeepFontAE on unlabeled font images\n",
    "2. Extract encoder weights\n",
    "3. Load into DeepFont encoder (with layer mapping)\n",
    "4. Freeze pretrained weights\n",
    "5. Train classification layers on labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248d9919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare encoder architectures\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# DeepFontAE Encoder\n",
    "ax = axes[0]\n",
    "ax.axis(\"off\")\n",
    "ae_encoder_info = [\n",
    "    (\"Input\", \"(1, 105, 105)\"),\n",
    "    (\"Conv2d(64, 11x11, s=2)\", \"(64, 48, 48)\"),\n",
    "    (\"MaxPool2d(2x2)\", \"(64, 24, 24)\"),\n",
    "    (\"ReLU\", \"(64, 24, 24)\"),\n",
    "    (\"Conv2d(128, 5x5)\", \"(128, 24, 24)\"),\n",
    "    (\"MaxPool2d(2x2)\", \"(128, 12, 12)\"),\n",
    "    (\"ReLU\", \"(128, 12, 12)\"),\n",
    "]\n",
    "\n",
    "y = 0.9\n",
    "for name, shape in ae_encoder_info:\n",
    "    has_bn = False\n",
    "    box_color = \"#E8F4F8\"\n",
    "    ax.add_patch(\n",
    "        FancyBboxPatch(\n",
    "            (0.1, y - 0.05),\n",
    "            0.8,\n",
    "            0.08,\n",
    "            boxstyle=\"round,pad=0.01\",\n",
    "            edgecolor=\"black\",\n",
    "            facecolor=box_color,\n",
    "            linewidth=2,\n",
    "        )\n",
    "    )\n",
    "    ax.text(0.5, y, f\"{name}\\n{shape}\", ha=\"center\", va=\"center\", fontsize=9, fontweight=\"bold\")\n",
    "    y -= 0.12\n",
    "\n",
    "ax.text(\n",
    "    0.5, 0.98, \"DeepFontAE Encoder\\n(No Batch Norm)\", ha=\"center\", fontsize=12, fontweight=\"bold\"\n",
    ")\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# DeepFont Encoder\n",
    "ax = axes[1]\n",
    "ax.axis(\"off\")\n",
    "df_encoder_info = [\n",
    "    (\"Input\", \"(1, 105, 105)\"),\n",
    "    (\"Conv2d(64, 11x11, s=2)\", \"(64, 48, 48)\"),\n",
    "    (\"BatchNorm2d(64)\", \"(64, 48, 48)\"),\n",
    "    (\"MaxPool2d(2x2)\", \"(64, 24, 24)\"),\n",
    "    (\"ReLU\", \"(64, 24, 24)\"),\n",
    "    (\"Conv2d(128, 5x5)\", \"(128, 24, 24)\"),\n",
    "    (\"BatchNorm2d(128)\", \"(128, 24, 24)\"),\n",
    "    (\"MaxPool2d(2x2)\", \"(128, 12, 12)\"),\n",
    "    (\"ReLU\", \"(128, 12, 12)\"),\n",
    "]\n",
    "\n",
    "y = 0.9\n",
    "for i, (name, shape) in enumerate(df_encoder_info):\n",
    "    has_bn = \"BatchNorm\" in name\n",
    "    box_color = \"#FFE8E8\" if has_bn else \"#E8F4F8\"\n",
    "    height = 0.06 if i < len(df_encoder_info) - 1 else 0.08\n",
    "    ax.add_patch(\n",
    "        FancyBboxPatch(\n",
    "            (0.1, y - height / 2),\n",
    "            0.8,\n",
    "            height,\n",
    "            boxstyle=\"round,pad=0.005\",\n",
    "            edgecolor=\"red\" if has_bn else \"black\",\n",
    "            facecolor=box_color,\n",
    "            linewidth=2 if has_bn else 1.5,\n",
    "        )\n",
    "    )\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        y,\n",
    "        f\"{name}\\n{shape}\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=8 if has_bn else 9,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    y -= 0.09\n",
    "\n",
    "ax.text(\n",
    "    0.5, 0.98, \"DeepFont Encoder\\n(With Batch Norm)\", ha=\"center\", fontsize=12, fontweight=\"bold\"\n",
    ")\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.suptitle(\"Encoder Comparison: DeepFontAE vs DeepFont\", fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "save_path = assets_dir / \"encoder_comparison.png\"\n",
    "plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Saved visualization to: {save_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b737f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display parameter statistics\n",
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Parameter Statistics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# DeepFontAE\n",
    "ae_total, ae_trainable = count_parameters(ae_model)\n",
    "print(f\"\\nDeepFontAE:\")\n",
    "print(f\"  Total parameters:     {ae_total:,}\")\n",
    "print(f\"  Trainable parameters: {ae_trainable:,}\")\n",
    "print(f\"  Memory (float32):     ~{ae_total * 4 / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# DeepFont\n",
    "df_total, df_trainable = count_parameters(classifier_model)\n",
    "print(f\"\\nDeepFont Classifier:\")\n",
    "print(f\"  Total parameters:     {df_total:,}\")\n",
    "print(f\"  Trainable parameters: {df_trainable:,}\")\n",
    "print(f\"  Memory (float32):     ~{df_total * 4 / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Encoder comparison\n",
    "ae_encoder_total = sum(p.numel() for p in ae_model.encoder.parameters())\n",
    "df_encoder_total = sum(p.numel() for p in classifier_model.encoder.parameters())\n",
    "print(f\"\\nEncoder Comparison:\")\n",
    "print(f\"  DeepFontAE encoder:   {ae_encoder_total:,} parameters\")\n",
    "print(f\"  DeepFont encoder:     {df_encoder_total:,} parameters\")\n",
    "print(f\"  Difference (BatchNorm): {df_encoder_total - ae_encoder_total:,} parameters\")\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Bar chart of total parameters\n",
    "models = [\"DeepFontAE\", \"DeepFont\\nClassifier\"]\n",
    "totals = [ae_total / 1e6, df_total / 1e6]  # Convert to millions\n",
    "colors = [\"#E8F4F8\", \"#E8F8E8\"]\n",
    "\n",
    "ax1.bar(models, totals, color=colors, edgecolor=\"black\", linewidth=2)\n",
    "ax1.set_ylabel(\"Parameters (Millions)\", fontsize=12, fontweight=\"bold\")\n",
    "ax1.set_title(\"Total Parameters Comparison\", fontsize=13, fontweight=\"bold\")\n",
    "ax1.grid(axis=\"y\", alpha=0.3)\n",
    "for i, v in enumerate(totals):\n",
    "    ax1.text(i, v + 0.5, f\"{v:.2f}M\", ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
    "\n",
    "# Pie chart for DeepFont breakdown\n",
    "df_encoder_params = df_encoder_total\n",
    "df_conv_params = sum(p.numel() for p in classifier_model.conv_part.parameters())\n",
    "df_fc_params = sum(p.numel() for p in classifier_model.fc_part.parameters())\n",
    "\n",
    "labels = [\"Encoder\", \"Conv Part\", \"FC Part\"]\n",
    "sizes = [df_encoder_params, df_conv_params, df_fc_params]\n",
    "colors_pie = [\"#E8F4F8\", \"#E8F8E8\", \"#F8E8F8\"]\n",
    "explode = (0.15, 0.15, 0.05)\n",
    "\n",
    "# Only show percentages on the pie chart\n",
    "wedges, texts, autotexts = ax2.pie(\n",
    "    sizes,\n",
    "    explode=explode,\n",
    "    labels=None,  # Don't show labels on pie\n",
    "    colors=colors_pie,\n",
    "    autopct=lambda pct: f\"{pct:.1f}%\" if pct > 2 else \"\",\n",
    "    shadow=True,\n",
    "    startangle=45,\n",
    "    textprops={\"fontsize\": 10, \"fontweight\": \"bold\"},\n",
    "    pctdistance=0.7,\n",
    ")\n",
    "\n",
    "# Add legend with full information\n",
    "legend_labels = [\n",
    "    f\"{labels[i]}: {sizes[i] / 1e6:.1f}M ({sizes[i] / sum(sizes) * 100:.1f}%)\"\n",
    "    for i in range(len(labels))\n",
    "]\n",
    "ax2.legend(\n",
    "    wedges,\n",
    "    legend_labels,\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1, 0, 0.5, 1),\n",
    "    fontsize=9,\n",
    ")\n",
    "\n",
    "ax2.set_title(\"DeepFont Parameter Distribution\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "save_path = assets_dir / \"parameter_statistics.png\"\n",
    "plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Saved visualization to: {save_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea2e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transfer learning workflow\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# Define workflow steps\n",
    "steps = [\n",
    "    {\n",
    "        \"title\": \"Step 1: Pretrain Autoencoder\",\n",
    "        \"desc\": \"Train DeepFontAE on unlabeled font images\\nusing reconstruction loss (MSE/L1)\",\n",
    "        \"y\": 0.85,\n",
    "        \"color\": \"#E8F4F8\",\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Step 2: Extract Encoder Weights\",\n",
    "        \"desc\": \"Save encoder weights from trained autoencoder\\n(Conv2d layers only, no decoder)\",\n",
    "        \"y\": 0.68,\n",
    "        \"color\": \"#FFF4E8\",\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Step 3: Load into Classifier\",\n",
    "        \"desc\": \"Initialize DeepFont encoder with pretrained weights\\nMap layer indices: 0→0, 3→4 (skip BatchNorm)\",\n",
    "        \"y\": 0.51,\n",
    "        \"color\": \"#FFE8E8\",\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Step 4: Freeze Encoder\",\n",
    "        \"desc\": \"Set requires_grad=False for pretrained layers\\nPreserves learned low-level features\",\n",
    "        \"y\": 0.34,\n",
    "        \"color\": \"#F8E8E8\",\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Step 5: Train Classifier\",\n",
    "        \"desc\": \"Fine-tune on labeled font dataset\\nOnly train: BatchNorm + Conv Part + FC Part\",\n",
    "        \"y\": 0.17,\n",
    "        \"color\": \"#E8F8E8\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Draw workflow boxes\n",
    "for i, step in enumerate(steps):\n",
    "    # Main box\n",
    "    box = FancyBboxPatch(\n",
    "        (0.1, step[\"y\"] - 0.06),\n",
    "        0.8,\n",
    "        0.12,\n",
    "        boxstyle=\"round,pad=0.01\",\n",
    "        edgecolor=\"black\",\n",
    "        facecolor=step[\"color\"],\n",
    "        linewidth=2,\n",
    "    )\n",
    "    ax.add_patch(box)\n",
    "\n",
    "    # Title\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        step[\"y\"] + 0.04,\n",
    "        step[\"title\"],\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    # Description\n",
    "    ax.text(\n",
    "        0.5, step[\"y\"] - 0.02, step[\"desc\"], ha=\"center\", va=\"center\", fontsize=9, style=\"italic\"\n",
    "    )\n",
    "\n",
    "    # Arrow to next step\n",
    "    if i < len(steps) - 1:\n",
    "        ax.annotate(\n",
    "            \"\",\n",
    "            xy=(0.5, steps[i + 1][\"y\"] + 0.06),\n",
    "            xytext=(0.5, step[\"y\"] - 0.06),\n",
    "            arrowprops=dict(arrowstyle=\"->\", lw=3, color=\"black\"),\n",
    "        )\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.title(\n",
    "    \"Transfer Learning Workflow: DeepFontAE → DeepFont\", fontsize=16, fontweight=\"bold\", pad=20\n",
    ")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "save_path = assets_dir / \"transfer_learning_workflow.png\"\n",
    "plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Saved visualization to: {save_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb546abb",
   "metadata": {},
   "source": [
    "## Step 6: Computational Graph Visualization (Optional)\n",
    "\n",
    "If `torchviz` is available, create computational graph visualizations:\n",
    "- Forward pass graph for DeepFontAE\n",
    "- Forward pass graph for DeepFont\n",
    "- Show gradient flow and connections between layers\n",
    "\n",
    "This provides a different perspective on the model architecture, focusing on the computational operations rather than just layer structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce68647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Try to create computational graph visualization\n",
    "try:\n",
    "    from torchviz import make_dot\n",
    "\n",
    "    # Create a sample input (ensure correct device)\n",
    "    device = next(ae_model.parameters()).device\n",
    "    sample_input = torch.randn(1, 1, 105, 105, device=device, requires_grad=True)\n",
    "\n",
    "    # Generate computational graph for DeepFontAE\n",
    "    print(\"Generating computational graph for DeepFontAE...\")\n",
    "    ae_output = ae_model(sample_input)\n",
    "    ae_graph = make_dot(\n",
    "        ae_output, params=dict(ae_model.named_parameters()), show_attrs=False, show_saved=False\n",
    "    )\n",
    "    ae_graph_path = assets_dir / \"deepfontae_graph\"\n",
    "    ae_graph.render(ae_graph_path, format=\"png\", cleanup=True)\n",
    "    print(f\"Saved DeepFontAE computational graph to: {ae_graph_path}.png\")\n",
    "\n",
    "    # Generate computational graph for DeepFont\n",
    "    print(\"Generating computational graph for DeepFont...\")\n",
    "    sample_input2 = torch.randn(1, 1, 105, 105, device=device, requires_grad=True)\n",
    "    clf_output = classifier_model(sample_input2)\n",
    "    clf_graph = make_dot(\n",
    "        clf_output,\n",
    "        params=dict(classifier_model.named_parameters()),\n",
    "        show_attrs=False,\n",
    "        show_saved=False,\n",
    "    )\n",
    "    clf_graph_path = assets_dir / \"deepfont_classifier_graph\"\n",
    "    clf_graph.render(clf_graph_path, format=\"png\", cleanup=True)\n",
    "    print(f\"Saved DeepFont computational graph to: {clf_graph_path}.png\")\n",
    "\n",
    "    print(\"Computational graphs created successfully!\")\n",
    "    print(\"Note: These graphs show the forward pass operations and parameter dependencies.\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"torchviz not available. Skipping computational graph visualization.\")\n",
    "    print(\"  To enable: pip install torchviz graphviz\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create computational graphs: {e}\")\n",
    "    print(\"  This is optional and doesn't affect the main visualizations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa9099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
